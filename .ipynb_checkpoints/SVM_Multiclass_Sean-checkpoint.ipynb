{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a logistic regression model in Python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Logistic%20Regression%20balanced.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seankamano/anaconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'statsmodels' has no attribute 'compat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7326b17a2ea5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpeakutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/statsmodels/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgraphics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgofplots\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mqqplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqqplot_2samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqqline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProbPlot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgraphics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgraphics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0memplike\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0memplike\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mduration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/statsmodels/stats/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m                                                   \u001b[0mTable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                                                   StratifiedTable)\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmediation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMediation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/statsmodels/stats/mediation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraphics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmaybe_name_or_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpdc\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'statsmodels' has no attribute 'compat'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt \n",
    "plt.rc(\"font\", size=14) \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "#from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import pytz\n",
    "import re\n",
    "\n",
    "import peakutils\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import requests\n",
    "\n",
    "#Read data from a local csv file:\n",
    "\n",
    "##Will change this to scrape files from the Smartfin.org website later.\n",
    "#data = pd.read_csv('Motion_13735.CSV', header=0)   \n",
    "#data = data.dropna()\n",
    "\n",
    "#Print out the column headings:\n",
    "#print(data.shape)\n",
    "#print(list(data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of specific ride IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ride_ids = ['15692']\n",
    "\n",
    "\n",
    "#ride_ids = ['14827']\n",
    "# 14743 - Motion Control July 10th\n",
    "# 14750 - Magnetometer Control July 11th\n",
    "# 14814 - Pool Displacement Control July 17th\n",
    "# 14815 - Compass Orientation (Lying on Charger Side) July 19th\n",
    "# 14816 - Orientation w Higher Sampling (Lying on Charger Side) July 20th\n",
    "# 14827 - Pool Displacement Control w Higher Sampling (Jul 23)\n",
    "# 14888 - First Buoy Calibration Experiment (July 30)\n",
    "# 15218 - Jasmine's Second Ride Sesh filmed with GoPro (Aug 29) //no footage\n",
    "# 15629 - Jasmine's First Ride Sesh filmed with VIRB (Oct. 24) //first labelled footage!\n",
    "# 15669 - Jasmine's Second Ride Sesh filmed with VIRB (Nov. 7) //second labelled footage!\n",
    "# 15692 - Jasmine's 3rd Ride Sesh filmed with VIRB (Nov. 9) //third labelled footage!\n",
    "# 15686 - Jasmine's 4th Ride Sesh filmed with VIRB (Nov. 11) //fourth labelled footage!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin ID Scraper (pulls dataframes for specific ride id from website):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%% Fin ID scraper\n",
    "# Input fin ID, get all ride IDs\n",
    "# base URL to which we'll append given fin IDs\n",
    "fin_url_base = 'http://surf.smartfin.org/fin/'\n",
    "\n",
    "# Look for the following text in the HTML contents in fcn below\n",
    "str_id_ride = 'rideId = \\'' # backslash allows us to look for single quote\n",
    "str_id_date = 'var date = \\'' # backslash allows us to look for single quote\n",
    "\n",
    "#%% Ride ID scraper\n",
    "# Input ride ID, get ocean and motion CSVs\n",
    "# Base URL to which we'll append given ride IDs\n",
    "ride_url_base = 'https://surf.smartfin.org/ride/'\n",
    "\n",
    "# Look for the following text in the HTML contents in fcn below\n",
    "str_id_csv = 'img id=\"temperatureChart\" class=\"chart\" src=\"' \n",
    "\n",
    "def get_csv_from_ride_id(rid):\n",
    "    # Build URL for each individual ride\n",
    "    ride_url = ride_url_base+str(rid)\n",
    "    print(ride_url)\n",
    "    \n",
    "    # Get contents of ride_url\n",
    "    html_contents = requests.get(ride_url).text\n",
    "    \n",
    "    # Find CSV identifier \n",
    "    loc_csv_id = html_contents.find(str_id_csv)\n",
    "    \n",
    "    # Different based on whether user logged in with FB or Google\n",
    "    offset_googleOAuth = [46, 114]\n",
    "    offset_facebkOAuth = [46, 112]\n",
    "    if html_contents[loc_csv_id+59] == 'f': # Facebook login\n",
    "        off0 = offset_facebkOAuth[0]\n",
    "        off1 = offset_facebkOAuth[1]\n",
    "    else: # Google login\n",
    "        off0 = offset_googleOAuth[0]\n",
    "        off1 = offset_googleOAuth[1]\n",
    "        \n",
    "    csv_id_longstr = html_contents[loc_csv_id+off0:loc_csv_id+off1]\n",
    "    \n",
    "#    print(csv_id_longstr)\n",
    "    \n",
    "    # Stitch together full URL for CSV\n",
    "    if (\"media\" in csv_id_longstr) & (\"Calibration\" not in html_contents): # other junk URLs can exist and break everything\n",
    "        \n",
    "        ocean_csv_url = 'https://surf.smartfin.org/'+csv_id_longstr+'Ocean.CSV'\n",
    "        motion_csv_url = 'https://surf.smartfin.org/'+csv_id_longstr+'Motion.CSV'\n",
    "        \n",
    "        print(ocean_csv_url)\n",
    "        # Go to ocean_csv_url and grab contents (theoretically, a CSV)\n",
    "        ocean_df_small = pd.read_csv(ocean_csv_url, parse_dates = [0])\n",
    "        elapsed_timedelta = (ocean_df_small['UTC']-ocean_df_small['UTC'][0])\n",
    "        ocean_df_small['elapsed'] = elapsed_timedelta/np.timedelta64(1, 's')\n",
    "        \n",
    "        motion_df_small = pd.read_csv(motion_csv_url, parse_dates = [0])\n",
    "        \n",
    "        # Reindex on timestamp if there are at least a few rows\n",
    "        if len(ocean_df_small) > 1:\n",
    "            ocean_df_small.set_index('UTC', drop = True, append = False, inplace = True)\n",
    "            motion_df_small.set_index('UTC', drop = True, append = False, inplace = True)\n",
    "            \n",
    "            #print(ocean_df_small)\n",
    "            #print(motion_df_small)\n",
    "            \n",
    "            #May need to change this sampling interval:\n",
    "            sample_interval = '33ms'\n",
    "            \n",
    "            \n",
    "            ocean_df_small_resample = ocean_df_small.resample(sample_interval).mean()\n",
    "            motion_df_small_resample = motion_df_small.resample(sample_interval).mean()\n",
    "            \n",
    "            # No need to save many extra rows with no fix\n",
    "            motion_df_small = motion_df_small[~np.isnan(motion_df_small.Latitude)]\n",
    "            \n",
    "            return ocean_df_small_resample, motion_df_small_resample\n",
    "\n",
    "    else:\n",
    "        ocean_df_small_resample = pd.DataFrame() # empty DF just so something is returned\n",
    "        motion_df_small_resample = pd.DataFrame() \n",
    "        return ocean_df_small_resample, motion_df_small_resample\n",
    "    \n",
    "appended_ocean_list = [] # list of DataFrames from original CSVs\n",
    "appended_motion_list = []\n",
    "appended_multiIndex = [] # fin_id & ride_id used to identify each DataFrame\n",
    "\n",
    "## Nested loops (for each fin ID, find all ride IDs, then build a DataFrame from all ride CSVs)\n",
    "## (Here, ride IDS are either ocean or motion dataframes)\n",
    "count_good_fins = 0\n",
    "    \n",
    "# Loop over ride_ids and find CSVs\n",
    "for rid in ride_ids:\n",
    "    try:\n",
    "        new_ocean_df, new_motion_df = get_csv_from_ride_id(rid) # get given ride's CSV from its ride ID using function above\n",
    "        #print(len(new_ocean_df))\n",
    "        #print(len(new_motion_df))\n",
    "        if not new_ocean_df.empty: # Calibration rides, for example\n",
    "            # Append only if DF isn't empty. There may be a better way to control empty DFs which are created above\n",
    "            appended_multiIndex.append(str(rid)) # build list to be multiIndex of future DataFrame\n",
    "            appended_ocean_list.append(new_ocean_df)\n",
    "            appended_motion_list.append(new_motion_df)\n",
    "            print(\"Ride data has been uploaded.\")\n",
    "            #print(\"Ride: \", rid, \"data has been uploaded.\")\n",
    "            count_good_fins += 1\n",
    "        \n",
    "    except: \n",
    "        print(\"Ride threw an exception!\")\n",
    "        #print(\"Ride \", rid, \"threw an exception!\")    \n",
    "\n",
    "#%% Build the \"Master\" DataFrame\n",
    "\n",
    "# appended_ocean_df.summary()\n",
    "df_keys = tuple(appended_multiIndex) # keys gotta be a tuple, a list which data in it cannot be changed\n",
    "ocean_df = pd.concat(appended_ocean_list, keys = df_keys, names=['ride_id'])\n",
    "motion_df = pd.concat(appended_motion_list, keys = df_keys, names = ['ride_id'])\n",
    "\n",
    "\n",
    "##Here, maybe just use info from the motion_df and don't worry about ocean_df data for now.\n",
    "##If you do want ocean_df data, look at how Phil was getting it from \"July 10th and 11th Calibration\" jupyter notebook file.\n",
    "\n",
    "#We can also check to see if the surfboard was recording \"in-water-freq\" or \n",
    "#\"out-of-water-freq\" based on how many NaN values we see. \n",
    "print(motion_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop the NA values from the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Drop the latitude and longitude values since most of them are Nan:\n",
    "motion_df_dropped = motion_df.drop(columns=['Latitude', 'Longitude'])\n",
    "\n",
    "\n",
    "#Drop the NAN values from the motion data:\n",
    "motion_df_dropped = motion_df_dropped.dropna(axis=0, how='any')\n",
    "print(motion_df_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an elapsed time field to sync Smartfin data with Video Footage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Create an elapsed_timedelta field:\n",
    "\n",
    "#timedelta_values = (motion_df_dropped['Time']-motion_df_dropped['Time'][0])\n",
    "#motion_df_dropped.insert(loc=1, column='TimeDelta', value=timedelta_values, drop=True)\n",
    "motion_df_dropped['TimeDelta'] = (motion_df_dropped['Time']-motion_df_dropped['Time'][0])\n",
    "#print(elapsed_timedelta)\n",
    "#motion_df_dropped.head()\n",
    "motion_df_dropped.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Footage sync code written by Alina:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Footage sync code written by Alina:\n",
    "\n",
    "import time\n",
    "\n",
    "#simple method: only walking, paddling, floating, surfing\n",
    "#complex method: columns created based on footage file labels\n",
    "def label_data( footage_file = 'Footage.txt', labelling_method = 'simple', sync_threshold = 20000 ):\n",
    "    \n",
    "    #First, perform sync\n",
    "    sync_buf = 0\n",
    "    with open(footage_file) as file:\n",
    "        for line in file:\n",
    "            labelled_time = line.split(None, 2) \n",
    "            try:\n",
    "                cur_time = time.strptime(labelled_time[0], '%M:%S')\n",
    "            except:\n",
    "                continue\n",
    "            labelled_time[1] = labelled_time[1].rstrip()\n",
    "            if labelled_time[1].lower() == 'sync': #Assumption that first word in sync line is \"sync\"\n",
    "                sync_time = cur_time.tm_min * 60 * 1000 + cur_time.tm_sec * 1000\n",
    "                index = 0\n",
    "                start = 0\n",
    "                end = 0\n",
    "                #Syncing occurs when IMU A2 data is negative for a longer period than the provided threshold\n",
    "                #Default is 20 seconds\n",
    "                for data in motion_df_dropped['IMU A2']:\n",
    "                    if data < 0 and start == 0:\n",
    "                        start = motion_df_dropped['TimeDelta'][index]\n",
    "                    elif data > 0 and start != 0:\n",
    "                        end = motion_df_dropped['TimeDelta'][index]\n",
    "                        if end - start > sync_threshold:\n",
    "                            sync_buf = start - sync_time\n",
    "                            break\n",
    "                        start = 0\n",
    "                    index += 1\n",
    "\n",
    "    accepted_labels = set()\n",
    "    if labelling_method == 'simple':\n",
    "        accepted_labels = {'WALKING', 'PADDLING', 'FLOATING', 'SURFING'}\n",
    "\n",
    "        #Create new DataFrame containing label info\n",
    "        label_frame = pd.DataFrame(0, index = motion_df_dropped.index, columns = accepted_labels)\n",
    "        for label in accepted_labels:\n",
    "            label_frame[label] = [0] * len(motion_df_dropped['Time'])\n",
    "    \n",
    "    #Convention of labelled footage text: \"MINUTE:SECOND LABEL\"\n",
    "    elapsed_time = 0\n",
    "    cur_label = ''\n",
    "    buffer = 0\n",
    "    with open(footage_file) as file:\n",
    "        for line in file:\n",
    "            \n",
    "            if labelling_method == 'simple':\n",
    "                labelled_time = line.split(None, 2) #simple categorizes on a one-word basis\n",
    "            else:\n",
    "                labelled_time = line.split(None, 1) #complex requires the entire label\n",
    "                \n",
    "            #If the first word is not a properly formatted time, the line cannot be read\n",
    "            try:\n",
    "                cur_time = time.strptime(labelled_time[0], '%M:%S')\n",
    "                cur_timeMS = cur_time.tm_min * 60 * 1000 + cur_time.tm_sec * 1000 + sync_buf\n",
    "            except:\n",
    "                continue\n",
    "            labelled_time[1] = labelled_time[1].rstrip() #Remove potential newline\n",
    "                \n",
    "            #Check for end of video and modify buffer accordingly\n",
    "            if labelled_time[1].lower() == 'end of video': #Assumption that label end video with \"end of video\"\n",
    "                buffer += cur_timeMS\n",
    "                \n",
    "            #Modify accepted labels list if reading a new label and in complex mode\n",
    "            elif labelling_method == 'complex' and (labelled_time[1].upper() not in accepted_labels):\n",
    "                accepted_labels.add(labelled_time[1].upper())\n",
    "                if not cur_label:\n",
    "                    label_frame = pd.DataFrame(0, index = motion_df_dropped.index, columns = accepted_labels)\n",
    "                label_frame[labelled_time[1].upper()] = [0] * len(motion_df_dropped['Time'])\n",
    "                \n",
    "            if labelled_time[1].upper() in accepted_labels:\n",
    "                while (elapsed_time < len(motion_df_dropped['Time']) and\n",
    "                      (np.isnan(motion_df_dropped['TimeDelta'][elapsed_time]) or\n",
    "                       motion_df_dropped['TimeDelta'][elapsed_time] < cur_timeMS + buffer)):\n",
    "                    if cur_label != '':\n",
    "                        label_frame[cur_label][elapsed_time] = 1\n",
    "                    elapsed_time += 1\n",
    "                if labelled_time[1].upper() != 'end of video':\n",
    "                    cur_label = labelled_time[1].upper()\n",
    "\n",
    "    labelled = pd.concat([motion_df_dropped, label_frame], axis = 1)\n",
    "\n",
    "    return labelled\n",
    "\n",
    "pd.options.display.max_rows = 5000\n",
    "pd.options.display.max_columns = 5000\n",
    "\n",
    "motion_df_simple = label_data('Footage3.txt')\n",
    "motion_df_simple.head(500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "motion_df_complex = label_data('Footage3.txt', 'complex')\n",
    "motion_df_complex.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correct IMU data\n",
    "\n",
    "#make a deep copy of motion_df_labelled\n",
    "df_converted = motion_df_complex.copy(deep = 'true')\n",
    "\n",
    "#for rows in df_corrected\n",
    "for row in range(0, df_converted.shape[0]):\n",
    "    \n",
    "    #convert acceleromters (new: m/s^2)\n",
    "    df_converted.iloc[row, df_converted.columns.get_loc('IMU A1')] *= 0.019141\n",
    "    df_converted.iloc[row, df_converted.columns.get_loc('IMU A2')] *= 0.019141\n",
    "    df_converted.iloc[row, df_converted.columns.get_loc('IMU A3')] *= 0.019141\n",
    " \n",
    "    #convert gyroscopes (new: deg/s)\n",
    "    df_converted.iloc[row, df_converted.columns.get_loc('IMU G1')] /= 8.2\n",
    "    df_converted.iloc[row, df_converted.columns.get_loc('IMU G2')] /= 8.2\n",
    "    df_converted.iloc[row, df_converted.columns.get_loc('IMU G3')] /= 8.2\n",
    "\n",
    "motion_df_complex.head(100)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_converted.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [50, 10]\n",
    "\n",
    "#define a function that plots a column of dataf in relation to time. color coded to match labels in dataf\n",
    "#requires that:\n",
    "#dataf has a 'TimeDelta' column\n",
    "#labels: walking, surfing, floating, paddling\n",
    "\n",
    "def createPlot (dataf, column):\n",
    "    \n",
    "        #create new data frame to be plotted\n",
    "        #Only consider columns after TimeDelta\n",
    "        dfPlot = pd.DataFrame(columns = ['TIME'] + list(dataf)[list(dataf).index('TimeDelta') + 1:], dtype = float)\n",
    "        \n",
    "        #add timedelta column from dataf to dfPlot\n",
    "        dfPlot['TIME'] = dataf['TimeDelta']\n",
    "        \n",
    "        #get the index of the column to be graphed\n",
    "        columnInd = dataf.columns.get_loc(column)\n",
    "        \n",
    "        #for each row in dfPlot (number of IMU readings)\n",
    "        for row in range(0, dfPlot.shape[0]):\n",
    "            \n",
    "            #for the indexes of the label columns in dfPlot\n",
    "            for col in range(1, dfPlot.shape[1]):\n",
    "                \n",
    "                #if a label in the row is 1 in dataf\n",
    "                if dataf.iloc[row, dataf.columns.get_loc(dfPlot.columns[col])] == 1:\n",
    "                    \n",
    "                    #add the sensors value to the corresponding column in dfPlot\n",
    "                    dfPlot.iloc[row, dfPlot.columns.get_loc(dfPlot.columns[col])] = dataf.iloc[row, columnInd]\n",
    "                    #dfPlot.iloc[row, dfPlot.columns.get]\n",
    "        \n",
    "        #Set up colormap so that we don't see a repeat in color when graphing\n",
    "        #plt.gca().set_prop_cycle('color',plt.cm.plasma(np.linspace(0,1,dfPlot.shape[1])))\n",
    "        plt.gca().set_prop_cycle('color',plt.cm.tab20(np.linspace(0,1,dfPlot.shape[1])))\n",
    "        for col in range (1, dfPlot.shape[1]):\n",
    "            plt.plot(dfPlot['TIME'], dfPlot[list(dfPlot)[col]])\n",
    "        \n",
    "        plt.gca().legend(loc = 'best')\n",
    "        plt.title(column)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"IMU Data\")\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Creating Plots\")\n",
    "createPlot(df_converted,'IMU A1')\n",
    "createPlot(df_converted,'IMU A2')\n",
    "createPlot(df_converted,'IMU A3')\n",
    "#createPlot(df_converted,'IMU G1')\n",
    "#createPlot(df_converted,'IMU G2')\n",
    "#createPlot(df_converted,'IMU G3')\n",
    "#createPlot(df_converted,'IMU M1')\n",
    "#createPlot(df_converted,'IMU M2')\n",
    "#createPlot(df_converted,'IMU M3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Butterworth Bandpass Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Apply a Filter to the signals to reduce noise:\n",
    "## Butter Filters for Bandpass:\n",
    "from scipy import signal\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = signal.butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def butter_bandpass_lfilter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = signal.lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "def butter_bandpass_filtfilt(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = signal.filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "##Butter Filters for Highpass:\n",
    "def butter_highpass(highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    high = highcut / nyq\n",
    "    b, a = signal.butter(order, high, btype='high')\n",
    "    return b, a\n",
    "\n",
    "def butter_highpass_lfilter(data, highcut, fs, order=5):\n",
    "    b, a = butter_lowpass(highcut, fs, order=order)\n",
    "    y = signal.lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "##Butter Filters for Lowpass:\n",
    "def butter_lowpass(lowcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    b, a = signal.butter(order, low, btype='low')\n",
    "    return b, a\n",
    "\n",
    "def butter_lowpass_lfilter(data, lowcut, fs, order=5):\n",
    "    b, a = butter_lowpass(lowcut, fs, order=order)\n",
    "    y = signal.lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the sample rate and the Low and High Cutoff frequencies\n",
    "#fs = 30\n",
    "fs = 5\n",
    "lowcut = 0.0333\n",
    "highcut = 1.5\n",
    "\n",
    "\n",
    "#Get each IMU column from the dataframe: \n",
    "#array = df_converted.values\n",
    "\n",
    "time_array = df_converted['TimeDelta'].values\n",
    "imua1_array = df_converted['IMU A1'].values\n",
    "imua2_array = df_converted['IMU A2'].values\n",
    "imua3_array = df_converted['IMU A3'].values\n",
    "\n",
    "\n",
    "\n",
    "##Graphing the bandpass filters:\n",
    "#A bandpass filter is both a highpass and a lowpass filter combined.\n",
    "butter_lfilter1 = butter_bandpass_lfilter(imua1_array, lowcut, highcut, fs, order=5)\n",
    "butter_lfilter2 = butter_bandpass_lfilter(imua2_array, lowcut, highcut, fs, order=5)\n",
    "butter_lfilter3 = butter_bandpass_lfilter(imua3_array, lowcut, highcut, fs, order=5)\n",
    "\n",
    "\n",
    "\n",
    "#butter_filtfilt = butter_bandpass_filtfilt(dacc_array1, lowcut, highcut, fs, order=5)\n",
    "\n",
    "\n",
    "\n",
    "#Can change num_elems to 1000 for example if you only want to graph the first 1000 elems:\n",
    "num_elems = len(time_array)\n",
    "\n",
    "print(\"Plotting:\")\n",
    "plt.figure(1)\n",
    "plt.subplot(311)\n",
    "plt.plot(time_array[:num_elems], butter_lfilter1[:num_elems])\n",
    "plt.title(\"Butterworth Bandpass Filtered IMU A1 Data\")\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.plot(time_array[:num_elems], butter_lfilter2[:num_elems])\n",
    "plt.title(\"Butterworth Bandpass Filtered IMU A2 Data\")\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.plot(time_array[:num_elems], butter_lfilter3[:num_elems])\n",
    "plt.title(\"Butterworth Bandpass Filtered IMU A3 Data\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"The plot for IMUA2 might be problematic because it truncates the sync step that happens in the beginning of the signal...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update dataframe with the filtered values, then do another labelled plot of all IMU values: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM on Raw data values: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Following this tutorial: \n",
    "#https://stackabuse.com/implementing-svm-and-kernel-svm-with-pythons-scikit-learn/\n",
    "\n",
    "#Also, make sure that the df_converted that gets copied does not get changed by the filtering methods being tested.\n",
    "dataset = df_converted.copy()\n",
    "\n",
    "print(dataset.shape)\n",
    "print(dataset.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide the data into attributes and labels, then training and testing sets. \n",
    "\n",
    "#Link which explains below: \n",
    "#https://stackoverflow.com/questions/37512079/python-pandas-why-does-df-iloc-1-values-for-my-training-data-select-till\n",
    "X = dataset.iloc[:, :11].values  #selects all attribute columns\n",
    "y = dataset.loc[:,'FLOATING'].values   #selects the column specified (ex: 'FLOATING')\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"y.shape:\", y.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
    "\n",
    "print(\"x_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"x_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "print(\"first few y_train values:\", y_train[0:10])\n",
    "print(\"first few y_test values:\", y_test[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (6688, 9)\n",
      "y.shape: (6688,)\n",
      "y: ['PADDLING' 'PADDLING' 'PADDLING' ... 'SURFING' 'SURFING' 'SURFING']\n",
      "X_train shape: (6019, 9)\n",
      "y_train shape: (6019,)\n",
      "X_test shape: (669, 9)\n",
      "y_test shape: (669,)\n"
     ]
    }
   ],
   "source": [
    "simple_multi_dataset = pd.read_csv('simple_multi_dataset.csv')\n",
    "\n",
    "X = simple_multi_dataset.iloc[:, 2:-1].values  #selects everything until the last column listed\n",
    "\n",
    "y = simple_multi_dataset.iloc[:, -1].values   #selects the last column \n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"y.shape:\", y.shape)\n",
    "\n",
    "print(\"y:\", y)\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing Data with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_resampled shape: (12936, 9)\n",
      "y_resampled shape: (12936,)\n",
      "X_train shape: (11642, 9)\n",
      "y_train shape: (11642,)\n",
      "X_test shape: (1294, 9)\n",
      "y_test shape: (1294,)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "X_resampled, y_resampled = SMOTE().fit_resample(X, y)\n",
    "print(\"X_resampled shape:\", X_resampled.shape)\n",
    "print(\"y_resampled shape:\", y_resampled.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size = 0.1, random_state = 0)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing Data with ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_resampled shape: (12852, 9)\n",
      "y_resampled shape: (12852,)\n",
      "X_train shape: (11566, 9)\n",
      "y_train shape: (11566,)\n",
      "X_test shape: (1286, 9)\n",
      "y_test shape: (1286,)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "X_resampled, y_resampled = ADASYN().fit_resample(X, y)\n",
    "print(\"X_resampled shape:\", X_resampled.shape)\n",
    "print(\"y_resampled shape:\", y_resampled.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size = 0.1, random_state = 0)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_id</th>\n",
       "      <th>UTC</th>\n",
       "      <th>IMU A1</th>\n",
       "      <th>IMU A2</th>\n",
       "      <th>IMU A3</th>\n",
       "      <th>IMU G1</th>\n",
       "      <th>IMU G2</th>\n",
       "      <th>IMU G3</th>\n",
       "      <th>IMU M1</th>\n",
       "      <th>IMU M2</th>\n",
       "      <th>IMU M3</th>\n",
       "      <th>simple_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15692</td>\n",
       "      <td>2018-11-09 19:17:41.238</td>\n",
       "      <td>0.344538</td>\n",
       "      <td>11.637728</td>\n",
       "      <td>2.775445</td>\n",
       "      <td>-2.195122</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>8.414634</td>\n",
       "      <td>-37.0</td>\n",
       "      <td>-165.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>PADDLING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15692</td>\n",
       "      <td>2018-11-09 19:17:41.502</td>\n",
       "      <td>-1.894959</td>\n",
       "      <td>10.087307</td>\n",
       "      <td>0.861345</td>\n",
       "      <td>16.585366</td>\n",
       "      <td>-4.146341</td>\n",
       "      <td>7.560976</td>\n",
       "      <td>-45.0</td>\n",
       "      <td>-163.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>PADDLING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15692</td>\n",
       "      <td>2018-11-09 19:17:41.733</td>\n",
       "      <td>-1.435575</td>\n",
       "      <td>10.699819</td>\n",
       "      <td>1.607844</td>\n",
       "      <td>1.829268</td>\n",
       "      <td>-6.219512</td>\n",
       "      <td>-4.390244</td>\n",
       "      <td>-42.0</td>\n",
       "      <td>-158.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>PADDLING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15692</td>\n",
       "      <td>2018-11-09 19:17:41.997</td>\n",
       "      <td>4.402430</td>\n",
       "      <td>8.288053</td>\n",
       "      <td>1.952382</td>\n",
       "      <td>-7.317073</td>\n",
       "      <td>-6.097561</td>\n",
       "      <td>3.292683</td>\n",
       "      <td>-41.0</td>\n",
       "      <td>-165.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>PADDLING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15692</td>\n",
       "      <td>2018-11-09 19:17:42.228</td>\n",
       "      <td>-1.856677</td>\n",
       "      <td>11.465459</td>\n",
       "      <td>2.392625</td>\n",
       "      <td>3.902439</td>\n",
       "      <td>-8.170732</td>\n",
       "      <td>20.121951</td>\n",
       "      <td>-46.0</td>\n",
       "      <td>-166.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>PADDLING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15692</td>\n",
       "      <td>2018-11-09 19:17:42.492</td>\n",
       "      <td>14.987403</td>\n",
       "      <td>21.839881</td>\n",
       "      <td>10.259576</td>\n",
       "      <td>-32.439024</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>2.195122</td>\n",
       "      <td>-47.0</td>\n",
       "      <td>-165.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>PADDLING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15692</td>\n",
       "      <td>2018-11-09 19:17:42.756</td>\n",
       "      <td>-0.229692</td>\n",
       "      <td>8.307194</td>\n",
       "      <td>0.363679</td>\n",
       "      <td>57.073171</td>\n",
       "      <td>22.682927</td>\n",
       "      <td>-15.487805</td>\n",
       "      <td>-48.0</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>PADDLING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15692</td>\n",
       "      <td>2018-11-09 19:17:42.987</td>\n",
       "      <td>-2.450048</td>\n",
       "      <td>12.652201</td>\n",
       "      <td>-1.186742</td>\n",
       "      <td>28.414634</td>\n",
       "      <td>-17.317073</td>\n",
       "      <td>5.853659</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>-160.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>PADDLING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15692</td>\n",
       "      <td>2018-11-09 19:17:43.251</td>\n",
       "      <td>-2.048087</td>\n",
       "      <td>8.364617</td>\n",
       "      <td>3.273111</td>\n",
       "      <td>-0.365854</td>\n",
       "      <td>5.975610</td>\n",
       "      <td>10.853659</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>-166.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>PADDLING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15692</td>\n",
       "      <td>2018-11-09 19:17:43.482</td>\n",
       "      <td>0.478525</td>\n",
       "      <td>10.604114</td>\n",
       "      <td>0.727358</td>\n",
       "      <td>-12.804878</td>\n",
       "      <td>-0.365854</td>\n",
       "      <td>4.756098</td>\n",
       "      <td>-56.0</td>\n",
       "      <td>-174.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>PADDLING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ride_id                      UTC     IMU A1     IMU A2     IMU A3  \\\n",
       "0    15692  2018-11-09 19:17:41.238   0.344538  11.637728   2.775445   \n",
       "1    15692  2018-11-09 19:17:41.502  -1.894959  10.087307   0.861345   \n",
       "2    15692  2018-11-09 19:17:41.733  -1.435575  10.699819   1.607844   \n",
       "3    15692  2018-11-09 19:17:41.997   4.402430   8.288053   1.952382   \n",
       "4    15692  2018-11-09 19:17:42.228  -1.856677  11.465459   2.392625   \n",
       "5    15692  2018-11-09 19:17:42.492  14.987403  21.839881  10.259576   \n",
       "6    15692  2018-11-09 19:17:42.756  -0.229692   8.307194   0.363679   \n",
       "7    15692  2018-11-09 19:17:42.987  -2.450048  12.652201  -1.186742   \n",
       "8    15692  2018-11-09 19:17:43.251  -2.048087   8.364617   3.273111   \n",
       "9    15692  2018-11-09 19:17:43.482   0.478525  10.604114   0.727358   \n",
       "\n",
       "      IMU G1     IMU G2     IMU G3  IMU M1  IMU M2  IMU M3 simple_label  \n",
       "0  -2.195122  -5.000000   8.414634   -37.0  -165.0   197.0     PADDLING  \n",
       "1  16.585366  -4.146341   7.560976   -45.0  -163.0   201.0     PADDLING  \n",
       "2   1.829268  -6.219512  -4.390244   -42.0  -158.0   216.0     PADDLING  \n",
       "3  -7.317073  -6.097561   3.292683   -41.0  -165.0   195.0     PADDLING  \n",
       "4   3.902439  -8.170732  20.121951   -46.0  -166.0   186.0     PADDLING  \n",
       "5 -32.439024   0.121951   2.195122   -47.0  -165.0   175.0     PADDLING  \n",
       "6  57.073171  22.682927 -15.487805   -48.0  -180.0   150.0     PADDLING  \n",
       "7  28.414634 -17.317073   5.853659   -50.0  -160.0   210.0     PADDLING  \n",
       "8  -0.365854   5.975610  10.853659   -50.0  -166.0   206.0     PADDLING  \n",
       "9 -12.804878  -0.365854   4.756098   -56.0  -174.0   194.0     PADDLING  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_multi_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SVM classifier.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#Training the algorithm: \n",
    "#The fit method is called to train the algorithm on the training data, which is passed as a parameter to the fit method.\n",
    "\n",
    "print(\"Running SVM classifier.\")\n",
    "\n",
    "\n",
    "#******Note:**********\n",
    "#** Also may need to scale the signals? So they influence the classifier the same....**\n",
    "# https://stats.stackexchange.com/questions/65094/why-scaling-is-important-for-the-linear-svm-classification\n",
    "\n",
    "\n",
    "\n",
    "#There are different types of kernel for SVMs, here we are using the \"linear\" kernel to see how it performs.\n",
    "from sklearn.svm import SVC  \n",
    "svclassifier = SVC(kernel='linear')  \n",
    "svclassifier.fit(X_train, y_train) \n",
    "\n",
    "#Making predictions: use the predict method of the SVC class\n",
    "y_pred = svclassifier.predict(X_test)  \n",
    "\n",
    "\n",
    "#This takes a really long time to run!\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[151  88 154]\n",
      " [ 68 251 143]\n",
      " [ 81  29 321]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    FLOATING       0.50      0.38      0.44       393\n",
      "    PADDLING       0.68      0.54      0.60       462\n",
      "     SURFING       0.52      0.74      0.61       431\n",
      "\n",
      "   micro avg       0.56      0.56      0.56      1286\n",
      "   macro avg       0.57      0.56      0.55      1286\n",
      "weighted avg       0.57      0.56      0.56      1286\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the algorithm, using a confusion matrix: \n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FLOATING': 3919, 'PADDLING': 3735, 'SURFING': 3912}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
