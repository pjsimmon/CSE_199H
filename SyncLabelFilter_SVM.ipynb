{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a logistic regression model in Python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Logistic%20Regression%20balanced.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt \n",
    "plt.rc(\"font\", size=14) \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "#from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import pytz\n",
    "import re\n",
    "\n",
    "import peakutils\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import requests\n",
    "\n",
    "#Read data from a local csv file:\n",
    "\n",
    "##Will change this to scrape files from the Smartfin.org website later.\n",
    "#data = pd.read_csv('Motion_13735.CSV', header=0)   \n",
    "#data = data.dropna()\n",
    "\n",
    "#Print out the column headings:\n",
    "#print(data.shape)\n",
    "#print(list(data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of specific ride IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ride_ids = ['15692']\n",
    "\n",
    "\n",
    "#ride_ids = ['14827']\n",
    "# 14743 - Motion Control July 10th\n",
    "# 14750 - Magnetometer Control July 11th\n",
    "# 14814 - Pool Displacement Control July 17th\n",
    "# 14815 - Compass Orientation (Lying on Charger Side) July 19th\n",
    "# 14816 - Orientation w Higher Sampling (Lying on Charger Side) July 20th\n",
    "# 14827 - Pool Displacement Control w Higher Sampling (Jul 23)\n",
    "# 14888 - First Buoy Calibration Experiment (July 30)\n",
    "# 15218 - Jasmine's Second Ride Sesh filmed with GoPro (Aug 29) //no footage\n",
    "# 15629 - Jasmine's First Ride Sesh filmed with VIRB (Oct. 24) //first labelled footage!\n",
    "# 15669 - Jasmine's Second Ride Sesh filmed with VIRB (Nov. 7) //second labelled footage!\n",
    "# 15692 - Jasmine's 3rd Ride Sesh filmed with VIRB (Nov. 9) //third labelled footage!\n",
    "# 15686 - Jasmine's 4th Ride Sesh filmed with VIRB (Nov. 11) //fourth labelled footage!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin ID Scraper (pulls dataframes for specific ride id from website):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%% Fin ID scraper\n",
    "# Input fin ID, get all ride IDs\n",
    "# base URL to which we'll append given fin IDs\n",
    "fin_url_base = 'http://surf.smartfin.org/fin/'\n",
    "\n",
    "# Look for the following text in the HTML contents in fcn below\n",
    "str_id_ride = 'rideId = \\'' # backslash allows us to look for single quote\n",
    "str_id_date = 'var date = \\'' # backslash allows us to look for single quote\n",
    "\n",
    "#%% Ride ID scraper\n",
    "# Input ride ID, get ocean and motion CSVs\n",
    "# Base URL to which we'll append given ride IDs\n",
    "ride_url_base = 'https://surf.smartfin.org/ride/'\n",
    "\n",
    "# Look for the following text in the HTML contents in fcn below\n",
    "str_id_csv = 'img id=\"temperatureChart\" class=\"chart\" src=\"' \n",
    "\n",
    "def get_csv_from_ride_id(rid):\n",
    "    # Build URL for each individual ride\n",
    "    ride_url = ride_url_base+str(rid)\n",
    "    print(ride_url)\n",
    "    \n",
    "    # Get contents of ride_url\n",
    "    html_contents = requests.get(ride_url).text\n",
    "    \n",
    "    # Find CSV identifier \n",
    "    loc_csv_id = html_contents.find(str_id_csv)\n",
    "    \n",
    "    # Different based on whether user logged in with FB or Google\n",
    "    offset_googleOAuth = [46, 114]\n",
    "    offset_facebkOAuth = [46, 112]\n",
    "    if html_contents[loc_csv_id+59] == 'f': # Facebook login\n",
    "        off0 = offset_facebkOAuth[0]\n",
    "        off1 = offset_facebkOAuth[1]\n",
    "    else: # Google login\n",
    "        off0 = offset_googleOAuth[0]\n",
    "        off1 = offset_googleOAuth[1]\n",
    "        \n",
    "    csv_id_longstr = html_contents[loc_csv_id+off0:loc_csv_id+off1]\n",
    "    \n",
    "#    print(csv_id_longstr)\n",
    "    \n",
    "    # Stitch together full URL for CSV\n",
    "    if (\"media\" in csv_id_longstr) & (\"Calibration\" not in html_contents): # other junk URLs can exist and break everything\n",
    "        \n",
    "        ocean_csv_url = 'https://surf.smartfin.org/'+csv_id_longstr+'Ocean.CSV'\n",
    "        motion_csv_url = 'https://surf.smartfin.org/'+csv_id_longstr+'Motion.CSV'\n",
    "        \n",
    "        print(ocean_csv_url)\n",
    "        # Go to ocean_csv_url and grab contents (theoretically, a CSV)\n",
    "        ocean_df_small = pd.read_csv(ocean_csv_url, parse_dates = [0])\n",
    "        elapsed_timedelta = (ocean_df_small['UTC']-ocean_df_small['UTC'][0])\n",
    "        ocean_df_small['elapsed'] = elapsed_timedelta/np.timedelta64(1, 's')\n",
    "        \n",
    "        motion_df_small = pd.read_csv(motion_csv_url, parse_dates = [0])\n",
    "        \n",
    "        # Reindex on timestamp if there are at least a few rows\n",
    "        if len(ocean_df_small) > 1:\n",
    "            ocean_df_small.set_index('UTC', drop = True, append = False, inplace = True)\n",
    "            motion_df_small.set_index('UTC', drop = True, append = False, inplace = True)\n",
    "            \n",
    "            #print(ocean_df_small)\n",
    "            #print(motion_df_small)\n",
    "            \n",
    "            #May need to change this sampling interval:\n",
    "            sample_interval = '33ms'\n",
    "            \n",
    "            \n",
    "            ocean_df_small_resample = ocean_df_small.resample(sample_interval).mean()\n",
    "            motion_df_small_resample = motion_df_small.resample(sample_interval).mean()\n",
    "            \n",
    "            # No need to save many extra rows with no fix\n",
    "            motion_df_small = motion_df_small[~np.isnan(motion_df_small.Latitude)]\n",
    "            \n",
    "            return ocean_df_small_resample, motion_df_small_resample\n",
    "\n",
    "    else:\n",
    "        ocean_df_small_resample = pd.DataFrame() # empty DF just so something is returned\n",
    "        motion_df_small_resample = pd.DataFrame() \n",
    "        return ocean_df_small_resample, motion_df_small_resample\n",
    "    \n",
    "appended_ocean_list = [] # list of DataFrames from original CSVs\n",
    "appended_motion_list = []\n",
    "appended_multiIndex = [] # fin_id & ride_id used to identify each DataFrame\n",
    "\n",
    "## Nested loops (for each fin ID, find all ride IDs, then build a DataFrame from all ride CSVs)\n",
    "## (Here, ride IDS are either ocean or motion dataframes)\n",
    "count_good_fins = 0\n",
    "    \n",
    "# Loop over ride_ids and find CSVs\n",
    "for rid in ride_ids:\n",
    "    try:\n",
    "        new_ocean_df, new_motion_df = get_csv_from_ride_id(rid) # get given ride's CSV from its ride ID using function above\n",
    "        #print(len(new_ocean_df))\n",
    "        #print(len(new_motion_df))\n",
    "        if not new_ocean_df.empty: # Calibration rides, for example\n",
    "            # Append only if DF isn't empty. There may be a better way to control empty DFs which are created above\n",
    "            appended_multiIndex.append(str(rid)) # build list to be multiIndex of future DataFrame\n",
    "            appended_ocean_list.append(new_ocean_df)\n",
    "            appended_motion_list.append(new_motion_df)\n",
    "            print(\"Ride data has been uploaded.\")\n",
    "            #print(\"Ride: \", rid, \"data has been uploaded.\")\n",
    "            count_good_fins += 1\n",
    "        \n",
    "    except: \n",
    "        print(\"Ride threw an exception!\")\n",
    "        #print(\"Ride \", rid, \"threw an exception!\")    \n",
    "\n",
    "#%% Build the \"Master\" DataFrame\n",
    "\n",
    "# appended_ocean_df.summary()\n",
    "df_keys = tuple(appended_multiIndex) # keys gotta be a tuple, a list which data in it cannot be changed\n",
    "ocean_df = pd.concat(appended_ocean_list, keys = df_keys, names=['ride_id'])\n",
    "motion_df = pd.concat(appended_motion_list, keys = df_keys, names = ['ride_id'])\n",
    "\n",
    "\n",
    "##Here, maybe just use info from the motion_df and don't worry about ocean_df data for now.\n",
    "##If you do want ocean_df data, look at how Phil was getting it from \"July 10th and 11th Calibration\" jupyter notebook file.\n",
    "\n",
    "#We can also check to see if the surfboard was recording \"in-water-freq\" or \n",
    "#\"out-of-water-freq\" based on how many NaN values we see. \n",
    "print(motion_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop the NA values from the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Drop the latitude and longitude values since most of them are Nan:\n",
    "motion_df_dropped = motion_df.drop(columns=['Latitude', 'Longitude'])\n",
    "\n",
    "\n",
    "#Drop the NAN values from the motion data:\n",
    "motion_df_dropped = motion_df_dropped.dropna(axis=0, how='any')\n",
    "print(motion_df_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an elapsed time field to sync Smartfin data with Video Footage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Create an elapsed_timedelta field:\n",
    "\n",
    "#timedelta_values = (motion_df_dropped['Time']-motion_df_dropped['Time'][0])\n",
    "#motion_df_dropped.insert(loc=1, column='TimeDelta', value=timedelta_values, drop=True)\n",
    "motion_df_dropped['TimeDelta'] = (motion_df_dropped['Time']-motion_df_dropped['Time'][0])\n",
    "#print(elapsed_timedelta)\n",
    "#motion_df_dropped.head()\n",
    "motion_df_dropped.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Footage sync code written by Alina:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Footage sync code written by Alina:\n",
    "\n",
    "import time\n",
    "\n",
    "#simple method: only walking, paddling, floating, surfing\n",
    "#complex method: columns created based on footage file labels\n",
    "def label_data( footage_file = 'Footage.txt', labelling_method = 'simple', sync_threshold = 20000 ):\n",
    "    \n",
    "    #First, perform sync\n",
    "    sync_buf = 0\n",
    "    with open(footage_file) as file:\n",
    "        for line in file:\n",
    "            labelled_time = line.split(None, 2) \n",
    "            try:\n",
    "                cur_time = time.strptime(labelled_time[0], '%M:%S')\n",
    "            except:\n",
    "                continue\n",
    "            labelled_time[1] = labelled_time[1].rstrip()\n",
    "            if labelled_time[1].lower() == 'sync': #Assumption that first word in sync line is \"sync\"\n",
    "                sync_time = cur_time.tm_min * 60 * 1000 + cur_time.tm_sec * 1000\n",
    "                index = 0\n",
    "                start = 0\n",
    "                end = 0\n",
    "                #Syncing occurs when IMU A2 data is negative for a longer period than the provided threshold\n",
    "                #Default is 20 seconds\n",
    "                for data in motion_df_dropped['IMU A2']:\n",
    "                    if data < 0 and start == 0:\n",
    "                        start = motion_df_dropped['TimeDelta'][index]\n",
    "                    elif data > 0 and start != 0:\n",
    "                        end = motion_df_dropped['TimeDelta'][index]\n",
    "                        if end - start > sync_threshold:\n",
    "                            sync_buf = start - sync_time\n",
    "                            break\n",
    "                        start = 0\n",
    "                    index += 1\n",
    "\n",
    "    accepted_labels = set()\n",
    "    if labelling_method == 'simple':\n",
    "        accepted_labels = {'WALKING', 'PADDLING', 'FLOATING', 'SURFING'}\n",
    "\n",
    "        #Create new DataFrame containing label info\n",
    "        label_frame = pd.DataFrame(0, index = motion_df_dropped.index, columns = accepted_labels)\n",
    "        for label in accepted_labels:\n",
    "            label_frame[label] = [0] * len(motion_df_dropped['Time'])\n",
    "    \n",
    "    #Convention of labelled footage text: \"MINUTE:SECOND LABEL\"\n",
    "    elapsed_time = 0\n",
    "    cur_label = ''\n",
    "    buffer = 0\n",
    "    with open(footage_file) as file:\n",
    "        for line in file:\n",
    "            \n",
    "            if labelling_method == 'simple':\n",
    "                labelled_time = line.split(None, 2) #simple categorizes on a one-word basis\n",
    "            else:\n",
    "                labelled_time = line.split(None, 1) #complex requires the entire label\n",
    "                \n",
    "            #If the first word is not a properly formatted time, the line cannot be read\n",
    "            try:\n",
    "                cur_time = time.strptime(labelled_time[0], '%M:%S')\n",
    "                cur_timeMS = cur_time.tm_min * 60 * 1000 + cur_time.tm_sec * 1000 + sync_buf\n",
    "            except:\n",
    "                continue\n",
    "            labelled_time[1] = labelled_time[1].rstrip() #Remove potential newline\n",
    "                \n",
    "            #Check for end of video and modify buffer accordingly\n",
    "            if labelled_time[1].lower() == 'end of video': #Assumption that label end video with \"end of video\"\n",
    "                buffer += cur_timeMS\n",
    "                \n",
    "            #Modify accepted labels list if reading a new label and in complex mode\n",
    "            elif labelling_method == 'complex' and (labelled_time[1].upper() not in accepted_labels):\n",
    "                accepted_labels.add(labelled_time[1].upper())\n",
    "                if not cur_label:\n",
    "                    label_frame = pd.DataFrame(0, index = motion_df_dropped.index, columns = accepted_labels)\n",
    "                label_frame[labelled_time[1].upper()] = [0] * len(motion_df_dropped['Time'])\n",
    "                \n",
    "            if labelled_time[1].upper() in accepted_labels:\n",
    "                while (elapsed_time < len(motion_df_dropped['Time']) and\n",
    "                      (np.isnan(motion_df_dropped['TimeDelta'][elapsed_time]) or\n",
    "                       motion_df_dropped['TimeDelta'][elapsed_time] < cur_timeMS + buffer)):\n",
    "                    if cur_label != '':\n",
    "                        label_frame[cur_label][elapsed_time] = 1\n",
    "                    elapsed_time += 1\n",
    "                if labelled_time[1].upper() != 'end of video':\n",
    "                    cur_label = labelled_time[1].upper()\n",
    "\n",
    "    labelled = pd.concat([motion_df_dropped, label_frame], axis = 1)\n",
    "\n",
    "    return labelled\n",
    "\n",
    "pd.options.display.max_rows = 5000\n",
    "pd.options.display.max_columns = 5000\n",
    "\n",
    "motion_df_simple = label_data('Footage3.txt')\n",
    "motion_df_simple.head(500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "motion_df_complex = label_data('Footage3.txt', 'complex')\n",
    "motion_df_complex.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correct IMU data\n",
    "\n",
    "#make a deep copy of motion_df_labelled\n",
    "df_converted = motion_df_complex.copy(deep = 'true')\n",
    "\n",
    "#for rows in df_corrected\n",
    "for row in range(0, df_converted.shape[0]):\n",
    "    \n",
    "    #convert acceleromters (new: m/s^2)\n",
    "    df_converted.iloc[row, df_converted.columns.get_loc('IMU A1')] *= 0.019141\n",
    "    df_converted.iloc[row, df_converted.columns.get_loc('IMU A2')] *= 0.019141\n",
    "    df_converted.iloc[row, df_converted.columns.get_loc('IMU A3')] *= 0.019141\n",
    " \n",
    "    #convert gyroscopes (new: deg/s)\n",
    "    df_converted.iloc[row, df_converted.columns.get_loc('IMU G1')] /= 8.2\n",
    "    df_converted.iloc[row, df_converted.columns.get_loc('IMU G2')] /= 8.2\n",
    "    df_converted.iloc[row, df_converted.columns.get_loc('IMU G3')] /= 8.2\n",
    "\n",
    "motion_df_complex.head(100)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_converted.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [50, 10]\n",
    "\n",
    "#define a function that plots a column of dataf in relation to time. color coded to match labels in dataf\n",
    "#requires that:\n",
    "#dataf has a 'TimeDelta' column\n",
    "#labels: walking, surfing, floating, paddling\n",
    "\n",
    "def createPlot (dataf, column):\n",
    "    \n",
    "        #create new data frame to be plotted\n",
    "        #Only consider columns after TimeDelta\n",
    "        dfPlot = pd.DataFrame(columns = ['TIME'] + list(dataf)[list(dataf).index('TimeDelta') + 1:], dtype = float)\n",
    "        \n",
    "        #add timedelta column from dataf to dfPlot\n",
    "        dfPlot['TIME'] = dataf['TimeDelta']\n",
    "        \n",
    "        #get the index of the column to be graphed\n",
    "        columnInd = dataf.columns.get_loc(column)\n",
    "        \n",
    "        #for each row in dfPlot (number of IMU readings)\n",
    "        for row in range(0, dfPlot.shape[0]):\n",
    "            \n",
    "            #for the indexes of the label columns in dfPlot\n",
    "            for col in range(1, dfPlot.shape[1]):\n",
    "                \n",
    "                #if a label in the row is 1 in dataf\n",
    "                if dataf.iloc[row, dataf.columns.get_loc(dfPlot.columns[col])] == 1:\n",
    "                    \n",
    "                    #add the sensors value to the corresponding column in dfPlot\n",
    "                    dfPlot.iloc[row, dfPlot.columns.get_loc(dfPlot.columns[col])] = dataf.iloc[row, columnInd]\n",
    "                    #dfPlot.iloc[row, dfPlot.columns.get]\n",
    "        \n",
    "        #Set up colormap so that we don't see a repeat in color when graphing\n",
    "        #plt.gca().set_prop_cycle('color',plt.cm.plasma(np.linspace(0,1,dfPlot.shape[1])))\n",
    "        plt.gca().set_prop_cycle('color',plt.cm.tab20(np.linspace(0,1,dfPlot.shape[1])))\n",
    "        for col in range (1, dfPlot.shape[1]):\n",
    "            plt.plot(dfPlot['TIME'], dfPlot[list(dfPlot)[col]])\n",
    "        \n",
    "        plt.gca().legend(loc = 'best')\n",
    "        plt.title(column)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"IMU Data\")\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Creating Plots\")\n",
    "createPlot(df_converted,'IMU A1')\n",
    "createPlot(df_converted,'IMU A2')\n",
    "createPlot(df_converted,'IMU A3')\n",
    "#createPlot(df_converted,'IMU G1')\n",
    "#createPlot(df_converted,'IMU G2')\n",
    "#createPlot(df_converted,'IMU G3')\n",
    "#createPlot(df_converted,'IMU M1')\n",
    "#createPlot(df_converted,'IMU M2')\n",
    "#createPlot(df_converted,'IMU M3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Butterworth Bandpass Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Apply a Filter to the signals to reduce noise:\n",
    "## Butter Filters for Bandpass:\n",
    "from scipy import signal\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = signal.butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def butter_bandpass_lfilter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = signal.lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "def butter_bandpass_filtfilt(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = signal.filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "##Butter Filters for Highpass:\n",
    "def butter_highpass(highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    high = highcut / nyq\n",
    "    b, a = signal.butter(order, high, btype='high')\n",
    "    return b, a\n",
    "\n",
    "def butter_highpass_lfilter(data, highcut, fs, order=5):\n",
    "    b, a = butter_lowpass(highcut, fs, order=order)\n",
    "    y = signal.lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "##Butter Filters for Lowpass:\n",
    "def butter_lowpass(lowcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    b, a = signal.butter(order, low, btype='low')\n",
    "    return b, a\n",
    "\n",
    "def butter_lowpass_lfilter(data, lowcut, fs, order=5):\n",
    "    b, a = butter_lowpass(lowcut, fs, order=order)\n",
    "    y = signal.lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the sample rate and the Low and High Cutoff frequencies\n",
    "#fs = 30\n",
    "fs = 5\n",
    "lowcut = 0.0333\n",
    "highcut = 1.5\n",
    "\n",
    "\n",
    "#Get each IMU column from the dataframe: \n",
    "#array = df_converted.values\n",
    "\n",
    "time_array = df_converted['TimeDelta'].values\n",
    "imua1_array = df_converted['IMU A1'].values\n",
    "imua2_array = df_converted['IMU A2'].values\n",
    "imua3_array = df_converted['IMU A3'].values\n",
    "\n",
    "\n",
    "\n",
    "##Graphing the bandpass filters:\n",
    "#A bandpass filter is both a highpass and a lowpass filter combined.\n",
    "butter_lfilter1 = butter_bandpass_lfilter(imua1_array, lowcut, highcut, fs, order=5)\n",
    "butter_lfilter2 = butter_bandpass_lfilter(imua2_array, lowcut, highcut, fs, order=5)\n",
    "butter_lfilter3 = butter_bandpass_lfilter(imua3_array, lowcut, highcut, fs, order=5)\n",
    "\n",
    "\n",
    "\n",
    "#butter_filtfilt = butter_bandpass_filtfilt(dacc_array1, lowcut, highcut, fs, order=5)\n",
    "\n",
    "\n",
    "\n",
    "#Can change num_elems to 1000 for example if you only want to graph the first 1000 elems:\n",
    "num_elems = len(time_array)\n",
    "\n",
    "print(\"Plotting:\")\n",
    "plt.figure(1)\n",
    "plt.subplot(311)\n",
    "plt.plot(time_array[:num_elems], butter_lfilter1[:num_elems])\n",
    "plt.title(\"Butterworth Bandpass Filtered IMU A1 Data\")\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.plot(time_array[:num_elems], butter_lfilter2[:num_elems])\n",
    "plt.title(\"Butterworth Bandpass Filtered IMU A2 Data\")\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.plot(time_array[:num_elems], butter_lfilter3[:num_elems])\n",
    "plt.title(\"Butterworth Bandpass Filtered IMU A3 Data\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"The plot for IMUA2 might be problematic because it truncates the sync step that happens in the beginning of the signal...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update dataframe with the filtered values, then do another labelled plot of all IMU values: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM on Raw data values: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Following this tutorial: \n",
    "#https://stackabuse.com/implementing-svm-and-kernel-svm-with-pythons-scikit-learn/\n",
    "\n",
    "#Also, make sure that the df_converted that gets copied does not get changed by the filtering methods being tested.\n",
    "dataset = df_converted.copy()\n",
    "\n",
    "print(dataset.shape)\n",
    "print(dataset.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide the data into attributes and labels, then training and testing sets. \n",
    "\n",
    "#Link which explains below: \n",
    "#https://stackoverflow.com/questions/37512079/python-pandas-why-does-df-iloc-1-values-for-my-training-data-select-till\n",
    "X = dataset.iloc[:, :11].values  #selects all attribute columns\n",
    "y = dataset.iloc[:, 19].values   #selects the labelled column that we care about\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"y.shape:\", y.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
    "\n",
    "print(\"x_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"x_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "print(\"first few y_train values:\", y_train[0:10])\n",
    "print(\"first few y_test values:\", y_test[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the algorithm: \n",
    "#The fit method is called to train the algorithm on the training data, which is passed as a parameter to the fit method.\n",
    "\n",
    "#There are different types of kernel for SVMs, here we are using the \"linear\" kernel to see how it performs.\n",
    "from sklearn.svm import SVC  \n",
    "svclassifier = SVC(kernel='linear')  \n",
    "svclassifier.fit(X_train, y_train) \n",
    "\n",
    "#Making predictions: use the predict method of the SVC class\n",
    "y_pred = svclassifier.predict(X_test)  \n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating the algorithm, using a confusion matrix: \n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
